services:
  archivebox:
    image: archivebox/archivebox:latest
    volumes:
      - ./data:/data
    labels:
      - traefik.enable=true
      - traefik.http.routers.archivebox.entrypoints=https
      - traefik.http.routers.archivebox.rule=Host(`subdomain.domain.com`)
      - traefik.http.routers.archivebox.tls=true
      - traefik.http.routers.archivebox.service=archivebox
      - traefik.http.services.archivebox.loadbalancer.server.port=8000
    environment:
      - ADMIN_USERNAME=ADMIN
      - ADMIN_PASSWORD=SOME_SECRET_PASSWORD
      - ALLOWED_HOSTS=* # restrict this to only accept incoming traffic via specific domain name
      - PUBLIC_INDEX=False # set to False to prevent anonymous users from viewing snapshot list
      - PUBLIC_SNAPSHOTS=False # set to False to prevent anonymous users from viewing snapshot content
      - PUBLIC_ADD_VIEW=False # set to True to allow anonymous users to submit new URLs to archive
      - SEARCH_BACKEND_ENGINE=sonic # tells ArchiveBox to use sonic container below for fast full-text search
      - SEARCH_BACKEND_HOST_NAME=sonic
      - SEARCH_BACKEND_PASSWORD=SOME_SECRET_PASSWORD_2
      - PUID=1000 # set to your host user's UID & GID if you encounter permissions issues
      - PGID=1000 # UID/GIDs <500 may clash with existing users and are not recommended
      - MEDIA_MAX_SIZE=750m # increase this filesize limit to allow archiving larger audio/video files
      - TIMEOUT=60 # increase this number to 120+ seconds if you see many slow downloads timing out
      - CHECK_SSL_VALIDITY=True # set to False to disable strict SSL checking (allows saving URLs w/ broken certs)
      - SAVE_ARCHIVE_DOT_ORG=False # set to False to disable submitting all URLs to Archive.org when archiving
      # ...
      # add further configuration options from archivebox/config.py as needed (to apply them only to this container)
      # or set using `docker compose run archivebox config --set SOME_KEY=someval` (to persist config across all containers)
      # For ad-blocking during archiving, uncomment this section and pihole service section below
    networks:
      - proxy


      ######## Optional Addons: tweak examples below as needed for your specific use case ########

      ### This optional container runs any scheduled tasks in the background, add new tasks like so:
      #   $ docker compose run archivebox schedule --add --every=day --depth=1 'https://example.com/some/rss/feed.xml'
      # then restart the scheduler container to apply any changes to the scheduled task list:
      #   $ docker compose restart archivebox_scheduler

  archivebox_scheduler:
    image: archivebox/archivebox:latest
    command: schedule --foreground --update --every=day
    environment:
      - TIMEOUT=120 # use a higher timeout than the main container to give slow tasks more time when retrying
      # - PUID=502                        # set to your host user's UID & GID if you encounter permissions issues
      # - PGID=20
    volumes:
      - ./data:/data
    # cpus: 2                               # uncomment / edit these values to limit scheduler container resource consumption
    # mem_limit: 2048m
    # restart: always


    ### This runs the optional Sonic full-text search backend (much faster than default rg backend).
    # If Sonic is ever started after not running for a while, update its full-text index by running:
    #   $ docker-compose run archivebox update --index-only

  sonic:
    image: valeriansaliou/sonic:latest
    build:
      # custom build just auto-downloads archivebox's default sonic.cfg as a convenience
      # not needed after first run / if you have already have ./etc/sonic.cfg present
      dockerfile_inline: >
        FROM quay.io/curl/curl:latest AS config_downloader

        RUN curl -fsSL 'https://raw.githubusercontent.com/ArchiveBox/ArchiveBox/stable/etc/sonic.cfg' > /tmp/sonic.cfg

        FROM valeriansaliou/sonic:latest

        COPY --from=config_downloader /tmp/sonic.cfg /etc/sonic.cfg
    expose:
      - 1491
    environment:
      - SEARCH_BACKEND_PASSWORD=SOME_SECRET_PASSWORD_2
    volumes:
      - ./sonic.cfg:/etc/sonic.cfg
      - ./data/sonic:/var/lib/sonic/store
  ### This container runs xvfb+noVNC so you can watch the ArchiveBox browser as it archives things,
  # or remote control it to set up a chrome profile w/ login credentials for sites you want to archive.
  # https://github.com/ArchiveBox/ArchiveBox/wiki/Chromium-Install#setting-up-a-chromium-user-profile

  novnc:
    image: theasp/novnc:latest
    environment:
      - DISPLAY_WIDTH=1920
      - DISPLAY_HEIGHT=1080
      - RUN_XTERM=no
    ports:
      # to view/control ArchiveBox's browser, visit: http://127.0.0.1:8080/vnc.html
      # restricted to access from localhost by default because it has no authentication
      - 127.0.0.1:8080:8080
  

  ### Example: Enable ability to run regularly scheduled archiving tasks by uncommenting this container
  #   $ docker compose run archivebox schedule --every=day --depth=1 'https://example.com/some/rss/feed.xml'
  # then restart the scheduler container to apply the changes to the schedule
  #   $ docker compose restart archivebox_scheduler

  # archivebox_scheduler:
  #    image: archivebox/archivebox:latest
  #    command: schedule --foreground
  #    environment:
  #        - MEDIA_MAX_SIZE=750m               # increase this number to allow archiving larger audio/video files
  #        # - TIMEOUT=60                      # increase if you see timeouts often during archiving / on slow networks
  #        # - ONLY_NEW=True                   # set to False to retry previously failed URLs when re-adding instead of skipping them
  #        # - CHECK_SSL_VALIDITY=True         # set to False to allow saving URLs w/ broken SSL certs
  #        # - SAVE_ARCHIVE_DOT_ORG=True       # set to False to disable submitting URLs to Archive.org when archiving
  #        # - PUID=502                        # set to your host user's UID & GID if you encounter permissions issues
  #        # - PGID=20
  #    volumes:
  #        - ./data:/data
  #        - ./etc/crontabs:/var/spool/cron/crontabs
  #    # cpus: 2                               # uncomment / edit these values to limit container resource consumption
  #    # mem_limit: 2048m
  #    # shm_size: 1024m


 

networks:
  proxy:
    external: true


    # To use remote storage for your ./data/archive (e.g. Amazon S3, Backblaze B2, Google Drive, OneDrive, SFTP, etc.)
    #   Follow the steps here to set up the Docker RClone Plugin https://rclone.org/docker/
    #     $ docker plugin install rclone/docker-volume-rclone:amd64 --grant-all-permissions --alias rclone
    #     $ nano /var/lib/docker-plugins/rclone/config/rclone.conf
    #     [examplegdrive]
    #     type = drive
    #     scope = drive
    #     drive_id = 1234567...
    #     root_folder_id = 0Abcd...
    #     token = {"access_token":...}

    # volumes:
    #     archive:
    #         driver: rclone
    #         driver_opts:
    #             remote: 'examplegdrive:archivebox'
    #             allow_other: 'true'
    #             vfs_cache_mode: full
    #             poll_interval: 0
